{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e8692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import whisper, json, datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization', use_auth_token=True)\n",
    "# Check si CUDA ok\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(f'{datetime.datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "pathfile=\"./AlainWOrk/wav/i.wav\"\n",
    "diarization_result = pipeline(pathfile)\n",
    "print(f'diarization_result ok in {datetime.datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "model = whisper.load_model(\"large-v2\")\n",
    "print(f\"Model chargé en {datetime.datetime.now() - start} Fait chauffer le gpu Marcel ! \")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "whisper_res = model.transcribe(pathfile, verbose=True, language=\"French\")\n",
    "print(f\"whisper_res OK in {datetime.datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850cf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Segment, Annotation, Timeline\n",
    "\n",
    "\n",
    "def get_text_with_timestamp(transcribe_res):\n",
    "    timestamp_texts = []\n",
    "    for item in transcribe_res['segments']:\n",
    "        start = item['start']\n",
    "        end = item['end']\n",
    "        text = item['text']\n",
    "        timestamp_texts.append((Segment(start, end), text))\n",
    "    return timestamp_texts\n",
    "\n",
    "\n",
    "def add_speaker_info_to_text(timestamp_texts, ann):\n",
    "    spk_text = []\n",
    "    for seg, text in timestamp_texts:\n",
    "        spk = ann.crop(seg).argmax()\n",
    "        spk_text.append((seg, spk, text))\n",
    "    return spk_text\n",
    "\n",
    "\n",
    "def merge_cache(text_cache):\n",
    "    sentence = ''.join([item[-1] for item in text_cache])\n",
    "    spk = text_cache[0][1]\n",
    "    start = text_cache[0][0].start\n",
    "    end = text_cache[-1][0].end\n",
    "    return Segment(start, end), spk, sentence\n",
    "\n",
    "\n",
    "# PUNC_SENT_END = []\n",
    "PUNC_SENT_END = ['.', '?', '!']\n",
    "\n",
    "\n",
    "def merge_sentence(spk_text):\n",
    "    merged_spk_text = []\n",
    "    pre_spk = None\n",
    "    text_cache = []\n",
    "    for seg, spk, text in spk_text:\n",
    "        if spk != pre_spk and pre_spk is not None and len(text_cache) > 0:\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = [(seg, spk, text)]\n",
    "            pre_spk = spk\n",
    "\n",
    "        elif text[-1] in PUNC_SENT_END:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = []\n",
    "            pre_spk = spk\n",
    "        else:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            pre_spk = spk\n",
    "    if len(text_cache) > 0:\n",
    "        merged_spk_text.append(merge_cache(text_cache))\n",
    "    return merged_spk_text\n",
    "\n",
    "\n",
    "def diarize_text(transcribe_res, diarization_result):\n",
    "    timestamp_texts = get_text_with_timestamp(transcribe_res)\n",
    "    spk_text = add_speaker_info_to_text(timestamp_texts, diarization_result)\n",
    "    res_processed = merge_sentence(spk_text)\n",
    "    return res_processed\n",
    "\n",
    "\n",
    "def write_to_txt(spk_sent, file):\n",
    "    with open(file, 'w') as fp:\n",
    "        for seg, spk, sentence in spk_sent:\n",
    "            line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sentence}\\n'\n",
    "            fp.write(line)\n",
    "        \n",
    "        \n",
    "def add_space_before_punctuation(text):\n",
    "    text = re.sub(r'(\\w)([?!])', r'\\1 \\2', text)\n",
    "    return text\n",
    "\n",
    "def remove_space_and_tiret(text):\n",
    "    text = re.sub(r'^\\ ', '',text)\n",
    "    text = re.sub(r'^\\— ', '',text)\n",
    "    return text\n",
    "\n",
    "def replace_words(text):\n",
    "\n",
    "    word_dict = {\n",
    "        \"que l'on doit\": \"que nous devons\",\n",
    "        \"on propose\": \"nous proposons\",\n",
    "        \"on restait\": \"nous restions\",\n",
    "        \"on restera\": \"nous resterons\",\n",
    "        \"on reste\": \"nous restons\",\n",
    "        \"on reprend\": \"nous reprenons\",\n",
    "        \"qu'on s'était\": \"que nous nous étions\",\n",
    "        \"qu'on s'est\": \"que nous nous sommes\",\n",
    "        \"on se rend\": \"nous nous rendons\",\n",
    "        \"on vous suit\": \"nous vous suivons\",\n",
    "        \"on pourrait\": \"nous pourrions\",\n",
    "        \"on peut\": \"nous pouvons\",\n",
    "        \"on ne saura\": \"nous ne saurons\",\n",
    "        \"on peut s'en\": \"nous pouvons nous en \",\n",
    "        \"il va falloir\": \"il faudra\",\n",
    "        \"on n'avait\": \"nous n'avions \",\n",
    "        \"qu'on ferait\": \"que nous ferions\",\n",
    "        \"c'est des\": \"ce sont des\",\n",
    "        \"c'était pas\": \"ce n'était pas\",\n",
    "        \"j'ai pas\": \"je n'ai pas\",\n",
    "        \"il a pas \": \"il n'a pas\",\n",
    "        \"c'est des\": \"ce sont des\",\n",
    "        \"qu'on n'est plus\": \"que nous ne sommes plus\",\n",
    "        \"qu'on n'est\": \"que nous sommes\",\n",
    "        \"qu'on est\": \"que nous sommes\",\n",
    "        \"qu'on a\": \"que nous avons\",\n",
    "        \"qu'on n'a plus\": \"que nous n'avons plus\",\n",
    "        \"on en avait\": \"nous en avions\",\n",
    "        \"on en a\": \"nous en avons\",\n",
    "        \"on tenait\": \"nous tenions\",\n",
    "        \"il faut pas\": \"il ne faut pas\",\n",
    "        \"il doit pas\": \"il ne doit pas\",\n",
    "        \"il travaille pas\": \"il ne travaille pas\",\n",
    "        \"on n'est pas\": \"nous ne sommes pas\",\n",
    "        \"qu'on puisse\": \"que nous puissions\",\n",
    "        \"est-ce que vous avez\": \"avez-vous\",\n",
    "        \"est-ce que vous êtes \": \"etes-vous\",\n",
    "        \"est-ce qu'il serait\": \"serait-il\",\n",
    "        \"on ne s'est pas\": \"nous ne nous sommes pas\",\n",
    "        \"qu'on n'était pas\": \"que nous n'étions pas\",\n",
    "        \"qu'on était pas\": \"que nous n'étions pas\",\n",
    "        \"qu'on ne mettait pas\": \"que nous ne mettions pas\",\n",
    "        \"qu'on mettait pas\": \"que nous ne mettions pas\",\n",
    "        \"on peut se\": \"nous pouvons nous\",\n",
    "        \"qu'on en a\": \"que nous en avons\",\n",
    "        \"est-ce qu'on peut\": \"pouvons-nous\",\n",
    "        \"on a\": \"nous avons\",\n",
    "        \"on est\": \"nous sommes\",\n",
    "        \"on vient\": \"nous venons\",\n",
    "        \"on va\": \"nous allons\",\n",
    "        \"on obtient\": \"nous obtenons\",\n",
    "        \"on fait\": \"nous faisons\",\n",
    "        \"on coordonne\": \"nous coordonnons\",\n",
    "        \"on se frotte\": \"nous nous frottons\",\n",
    "        \"on se donne\": \"nous nous donnons\",\n",
    "        \"on mange\": \"nous mangeons\",\n",
    "        \"on donne\": \"nous donnons\",\n",
    "        \"on apprend\": \"nous apprenons\",\n",
    "        \"on s'est dit\": \"nous nous sommes dit\",\n",
    "        \"on sait\": \"nous savons\",\n",
    "        \"on prend\": \"nous prenons\",\n",
    "        \"ça\": \"cela\",\n",
    "        \"est-ce que vous avez\": \"avez-vous\",\n",
    "        \"est-ce qu'il y a\": \"y a-t-il \",\n",
    "        \"puisqu'on voit\": \"puisque nous voyons\",\n",
    "        \"qu'on voit\": \"que nous voyons\",\n",
    "        \"comment on peut\": \"comment pouvons-nous\",\n",
    "        \"on participe\": \"nous participons\",\n",
    "        \"-là\": \"\",\n",
    "        \"qu'nous\": \"que nous\",\n",
    "    }\n",
    "    \n",
    "\n",
    "    for old_word, new_word in word_dict.items():\n",
    "        pattern = r'\\b(' + old_word + r')\\b'                   \n",
    "        pattern_case_insensitive = re.compile(pattern, re.IGNORECASE)\n",
    "        text = remove_space_and_tiret(text)\n",
    "        text = add_space_before_punctuation(text)\n",
    "        text = re.sub(pattern_case_insensitive, lambda m: new_word.capitalize() if m.group(1)[0].isupper() else new_word.lower(), text)\n",
    "    return text                                                             \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement des resultats pour plus tard !\n",
    "import pathlib, os, json\n",
    "path = pathlib.Path(pathfile)\n",
    "dir_result = f\"{path.parent.absolute()}/{path.name}_result\"\n",
    "\n",
    "pathlib.Path(f\"{dir_result}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.listdir(path.parent.absolute())\n",
    "\n",
    "with open(f\"{dir_result}/{path.name}.whisper.json\", 'w') as f:\n",
    "    json.dump(whisper_res, f, ensure_ascii=False, indent=4)\n",
    "f.close()\n",
    "    \n",
    "with open(f\"{dir_result}/{path.name}.diarization.json\", 'w') as d:\n",
    "    json.dump(diarization_result.for_json(), d, ensure_ascii=False, indent=4)\n",
    "d.close()\n",
    "\n",
    "with open(f\"{dir_result}/{path.name}.diarization.txt\", 'w') as t:\n",
    "    t.write(f\"{diarization_result}\")\n",
    "t.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da742e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "final_result = diarize_text(whisper_res, diarization_result)\n",
    "with open(f\"{dir_result}/{path.name}.final.txt\", 'w') as r:\n",
    "    ex_speaker = None\n",
    "    for segment, speaker, text in final_result:\n",
    "        if speaker != ex_speaker:\n",
    "            r.write('\\n\\n')\n",
    "            r.write(f'{speaker}')\n",
    "            r.write('\\n\\n')\n",
    "            ex_speaker = speaker\n",
    "        r.write(replace_words(text))\n",
    "\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895dfae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# tokenizer(final_result)['input_ids']\n",
    "\n",
    "bloc = \"\"\n",
    "list_bloc = []\n",
    "\n",
    "ex_speaker = None\n",
    "for segment, speaker, text in final_result:\n",
    "    if speaker != ex_speaker:\n",
    "        len_token = len(tokenizer(bloc)['input_ids'])\n",
    "        if len_token > 3000 :\n",
    "            list_bloc.append(bloc)\n",
    "            bloc = \"\"\n",
    "        bloc += '\\n\\n'\n",
    "        bloc += f'{speaker} : '\n",
    "        ex_speaker = speaker\n",
    "    bloc += replace_words(text)\n",
    "\n",
    "print(len(list_bloc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = \"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=f\"Résume moi chaque intervention : \\n{list_bloc[0]}\",\n",
    "  temperature=0,\n",
    "  max_tokens=1000,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "text = response.get('choices')[0].get('text')\n",
    "text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
